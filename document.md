　　
# Summer Intern 2019
当インターンは、クロスパワーアプリ部門インターン2019「検索サイトを作ってみよう」です。
<!-- # 0. 目次
1. 概要

2. 環境準備手順
3. Python課題一覧
4. スクレイピング課題一覧 -->

# 1. 概要
## 目的
**●　クローリング・スクレイピングができるようになる**  

検索サイトを作るんじゃないのか？という方はご安心を。  
この二つは検索サイトのキーとなる技術なのです。   
また、この二つは検索サイトの作成に役立つだけでなく、  
個人的な情報収集にも多いに役立つことも知られています。    
この研修を通じて、参加者が圧倒的情報強者への第一歩を踏み出していただけることを期待しています。  

![仕事 できる男](img/bannou.png)←弊社イメージ

**●　Pythonが使えるようになる**  
Pythonが使えたら素敵だねっていう内容…  
***
## 検索サイトについて
**「検索サイトの裏では無数のロボットが走っている。」** と言ったら信じますか？  
![ねこ ルンバ](img/neko_runba.png)

有名な検索サイトとして、GoogleやYahoo、Bingなどが挙げられますが  
これらどのサイトの裏でもクローラと呼ばれるロボットが日々駆け回っています。  
（残念ながら在り物のロボットが跳んだり跳ねたりしているわけではないのですが…）  
今回はこのクローラと呼ばれる謎のロボット?を知り、参加者にはこのロボットが作れるようになってもらいます。

それでは改めまして今回のテーマ、検索の仕組みをざっくりと説明しますと、  
　　「クローラがサイトを巡回してページ情報を分解、インデックスにして登録している。」  
.....？？
  
順番に見ていきましょう  
まずは予備知識として「クローラ」を簡単に説明します。  

「クローラ」とは  

複数のURLを与えられ、それらのURLからページの中でほしい情報を抽出して回るロボットです  
(クローリングしてくれるロボットで別名"スパイダー")

※特に検索サイトのクローラはあるドメインについてまずはrobots.txtというファイルを参照します。  
　これには、サイト内のマップと進入禁止場所が書かれており、許可された場所のみクローリングする仕様になっています。  
![サイト内マップ](img/mapman.png)
　

これを踏まえて、改めて検索の仕組みを説明しましょう。  
１．ドメインを基にページのrobots.txtを開く。サイトマップなどを取得。  
２．各ページを巡回して、文章などを単語レベルで切り分けてインデックスに保存する。  
３．併せてその他サイトの新しさなどのシグナルも保存する。  

これだけです。しかし、この3番にGoogleの検索順位などが関わっており、一般的にSEOと呼ばれています。  
＜参考＞  
[SEO（Search Engine Optimization:検索エンジン最適化）](https://moukegaku.com/google-ranking-algorithm/#domain)  



以上のようにクローラが必死に駆け回ってくれているから、我々は検索することができるんですね～。
  


***
## スクレイピングについて
  スクレイピングとは「ページのhtmlから情報を収集・抽出すること」です。  

スクレイピングとクローリングの違いはなんでしょうか？  
(赤い人と青い人的な画像を入れる)  
シンプルに対象となるページ数の違いです。  
スクレイピングは主に一つのページから情報を抽出することを言いますが、  
クローリングは複数のページから情報を抽出して回る行為のことを指します。
# 2. 環境準備手順
環境準備は以下の手順に則って行うこと。また詳細は当日追加で説明を行う。
1. [サマーインターンリポジトリ](https://github.com/crosspower/summer_intern)
にて「clone or download」ボタンを押下し、URLをコピー
2. 「git clone [**URL**]」をVSCode上で実行し、ファイルを一括で取得
    (summer_intern2019ディレクトリに移動)
3. 「py -m venv intern」で"intern"という名前で仮想環境を作成する
4. 「. intern/Scripts/activate」で仮想環境に入る
5. 「pip install -r requirements.txt」でモジュールを一括インストール
6. frontendディレクトリに移動し、「npm install」でモジュールを一括インストール
7. 「npm run serve」でフロントエンドのサーバーを起動
8. 「python main.py」でバックエンドのサーバーを起動
9. 以上で画面から操作確認を行い、環境準備完了

# 3. Python課題一覧

# 4. スクレイピング課題一覧